{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_identity = pd.read_csv(r'.\\ieee-fraud-detection\\train_identity.csv')\n",
    "train_transaction = pd.read_csv(r'.\\ieee-fraud-detection\\train_transaction.csv')\n",
    "test_identity = pd.read_csv(r'.\\ieee-fraud-detection\\test_identity.csv')\n",
    "test_transaction = pd.read_csv(r'.\\ieee-fraud-detection\\test_transaction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left join on TransactionID, store in train\n",
    "train = pd.merge(train_transaction, train_identity, on = 'TransactionID', how = 'left')\n",
    "test = pd.merge(test_transaction, test_identity, on = 'TransactionID', how = 'left')\n",
    "train_label = train['isFraud']\n",
    "train = train.drop(columns = ['isFraud'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train, test], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{dtype('int64'), dtype('float64'), dtype('O')}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a dictionary to store all dtypes of all features\n",
    "dtype_dict = {}\n",
    "for i in all_data.columns.tolist():\n",
    "    dtype_dict[i]= train[i].dtype\n",
    "# print unique data type\n",
    "set(dtype_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary to store all feature type\n",
    "# we know int and float are continuous O is categorical\n",
    "# cont for continuous\n",
    "# cat for catagorical\n",
    "feature_type = {}\n",
    "for i in dtype_dict.keys():\n",
    "    if dtype_dict[i] == 'int64':\n",
    "        feature_type[i] = 'cont'\n",
    "    if dtype_dict[i] == 'float64':\n",
    "        feature_type[i] = 'cont'\n",
    "    if dtype_dict[i] == 'O':\n",
    "        feature_type[i] = 'cat'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find percentage of non null value of each feature\n",
    "null = {}\n",
    "for i in all_data.columns.tolist():\n",
    "    null[i] = all_data[i].isnull().value_counts()[False]/all_data.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_value(df):\n",
    "    feature_unique_value = {}\n",
    "    for i in df.columns.tolist():\n",
    "        if feature_type[i] == 'cat':\n",
    "            feature_unique_value[i]=len(set(all_data[i].unique()))\n",
    "    return feature_unique_value\n",
    "\n",
    "def reduce_f_dim(col, critical_value):\n",
    "    temp = col.value_counts().to_dict()\n",
    "    total = sum(temp.values())\n",
    "    new_col = []\n",
    "    for keys in col:\n",
    "        if temp[keys] / total > critical_value:\n",
    "            new_col.append(keys)\n",
    "        else:\n",
    "            new_col.append('etc')\n",
    "    return new_col\n",
    "\n",
    "def categorical_feature_reduce_dim(df, critical_feature_num, critical_value):\n",
    "    \n",
    "    feature_unique_value = find_unique_value(all_data)\n",
    "    for i in feature_unique_value.keys():\n",
    "        if feature_unique_value[i] > critical_feature_num:\n",
    "            col = df[i]\n",
    "            fill_value = 'null'\n",
    "            col = col.fillna(fill_value)\n",
    "            col = reduce_f_dim(col, critical_value)\n",
    "            df[i] = col\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_date(df):\n",
    "    startdate = datetime.datetime.strptime('20190101', '%Y%m%d')\n",
    "    df['Date'] = df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n",
    "    df['Week'] = df['Date'].dt.dayofweek\n",
    "    df['days'] = df['Date'].dt.day\n",
    "    df['hours'] = df['Date'].dt.hour\n",
    "    df = df.drop(columns=['Date'])\n",
    "    return df\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = categorical_feature_reduce_dim(train, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            null\n",
       "1            null\n",
       "2            null\n",
       "3            null\n",
       "4             etc\n",
       "5            null\n",
       "6            null\n",
       "7            null\n",
       "8             etc\n",
       "9            null\n",
       "10        Windows\n",
       "11           null\n",
       "12           null\n",
       "13           null\n",
       "14           null\n",
       "15           null\n",
       "16            etc\n",
       "17        Windows\n",
       "18           null\n",
       "19           null\n",
       "20           null\n",
       "21           null\n",
       "22           null\n",
       "23           null\n",
       "24           null\n",
       "25           null\n",
       "26           null\n",
       "27           null\n",
       "28           null\n",
       "29           null\n",
       "           ...   \n",
       "590510       null\n",
       "590511       null\n",
       "590512       null\n",
       "590513       null\n",
       "590514       null\n",
       "590515       null\n",
       "590516       null\n",
       "590517       null\n",
       "590518       null\n",
       "590519       null\n",
       "590520       null\n",
       "590521        etc\n",
       "590522       null\n",
       "590523       null\n",
       "590524       null\n",
       "590525       null\n",
       "590526        etc\n",
       "590527       null\n",
       "590528       null\n",
       "590529        etc\n",
       "590530       null\n",
       "590531        etc\n",
       "590532       null\n",
       "590533       null\n",
       "590534        etc\n",
       "590535       null\n",
       "590536       null\n",
       "590537       null\n",
       "590538       null\n",
       "590539       null\n",
       "Name: DeviceInfo, Length: 590540, dtype: object"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp['DeviceInfo']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
